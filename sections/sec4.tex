\section{Adaptive Enrichment for Trust-Region Reduced Basis Approximation}\label{sec:AdapTRRBAlg}

The objective of this section is to describe the trust-region reduced basis method described in~\cite[Section 4]{Keil2021}.
To this end we shall stick to a general description, and only mention the relevant theoretical results for convergence at the end.
Throughout we shall choose the model function (cf. Subsection~\ref{subsec:TRRB}) to be defined as the sequence indexed by $k \geq 0$ as follows
\begin{equation*}\label{TRModelFunc}
    m^{(k)}(\eta) := \mathcal{J}_N{(k)}(\mu^{(k)} + \eta).
\end{equation*}
The reduced spaces are assumed to be initialized as $V_N^{pr, 0} = \{ u_\mu \}, V_N^{du, 0} = \{ p_\mu \}$, with $u_\mu$ and $p_\mu$ being the FOM solutions for some initial guess $\mu^{(0)} \in \mathcal{P}$.
We can then state the inexact reduced version of~\eqref{TROpti} as introduced in~\cite[Equation 51]{Qian2017}
\begin{equation}\label{TRInexactOpti}
    \min\limits_{s \in \mathcal{P}} \mathcal{J}_N^{(k)}(\mu^{(k)} + s) \quad \text{s.t.} \quad \frac{\Delta_{\mathcal{J}_N^{(k)}}(\mu^{(k)} + s)}{\mathcal{J}_N^{(k)}(\mu^{(k)} + s)} \leq \delta^{(k)},
\end{equation}
where the equality constraint $Res_{\mu^{(k)} + s}^{pr} (u_{N, \mu^{(k)} + s})[v] = 0$ is hidden in $\mathcal{J}_N^{(k)}$, and $\delta^{(k)}$ is the radius of the trust-region.

For every step of the trust-region method we then have to solve the local problem via BFGS.\@
For the first iteration we need to define the \textbf{approximate general Cauchy point} (AGC).
This point is defined by
\begin{equation*}\label{AGCPoint}
    \mu_{AGC}^{(k)} := \mu^{(k, 0)}(j^{(k, 0)}) = \mathbb{P}_{\mathcal{P}} \left( \mu^{(k, 0)} + \kappa^{j^{(k, 0)}} d^{(k, 0)} \right),
\end{equation*}
where $\mathbb{P}_\mathcal{P}$ is the projection ${\left( \mathbb{P}_\mathcal{P}(\mu) \right)}_i := \begin{cases}
    {(\mu_a)}_i, {(\mu)}_i \leq {(\mu_a)}_i, \\
    {(\mu)}_i, {(\mu_a)}_i \leq {(\mu)}_i \leq {(\mu_b)}_i, \\
    {(\mu_b)}_i, {(\mu_b)}_i \leq {(\mu)}_i,
\end{cases}$
mapping the parameters into the rectangular bounds $\mathcal{P}$ in analogy to the projection described in Subsection~\ref{subsec:TRRB}.
Here the initial descent direction is $d^{(k, 0)} = - \nabla_\mu \mathcal{J}_N^{(k)}(\mu^{(k, 0)})$, and $j^{(k)}$ is the smallest integer such that the Armijo condition in~\cite[Inequality 4.4]{Keil2021} and the trust-region inequality condition from~\eqref{TRInexactOpti} (cf.~\cite[Inequality 4.5]{Keil2021}).
We then iterate via
\begin{align*}
    \mu^{(k, l)}(j^{(k, l)}) &:= \mathbb{P}_\mathcal{P} \left( \mu^{(k, 0)} + \kappa^{j^{(k, l)}} d^{(k, 0)} \right), \\
    \mu^{(k, l+ 1)} &:= \mu^{(k, l)}(j_k^{(l)})
\end{align*}
with this $j$ once again satisfying the same conditions for the indices $k, l$.
After the termination condition for the BFGS method has been reached at $L$ iterations, see~\cite[Inequalities 4.6a and 4.6b]{Keil2021}, we set $\mu^{(k + 1)} := \mu^{(k, L)}$ for the next (potential) trust-region iteration.

As explained in~\cite{Keil2021}, the main drawback of the trust-region algorithm is that in the standard version the trust-radius may be significantly shrunk, thus missing out on the rate of convergence for the BFGS method.
To fix this, we define the threshold $\eta \in \interval[open right]{1}{\frac{3}{4}}$ and the criterion for enlarging the radius as proposed in~\cite[Inequality 4.7]{Keil2021} by
\begin{equation*}\label{EnlargingCriterion}
    \rho^{(k)} := \frac{\mathcal{J}(\mu^{(k)}) - \mathcal{J}(\mu^{(k + 1)})}{\mathcal{J}_N^{(k)}(\mu^{(k)}) - \mathcal{J}_N^{(k)}(\mu^{(k + 1)})} \geq \eta.
\end{equation*}
With this idea in hand, we can now formulate the adaptive trust-region algorithm in Algorithm~\ref{alg:AdaptTRRB}.

One critical point in this alorithm that we have yet to cover are lines~\ref{Update1} and~\ref{Update2}.
Here we just mentioned an update to the reduced spaces but so far have not yet specified what this means.
Essentially there are two options according to~\cite{Keil2021}:
\begin{itemize}
    \item Lagrangian reduced spaces: Construct the reduced space by adding the FOM solutions directly related to the primal and dual spaces, i.e.\@ for some $\mu \in \mathcal{P}$ from the optimization $V_N^{pr, k + 1} := V_N^{pr, k} \cup \{ u_\mu \}, V_N^{du, k + 1} := V_N^{du, k} \cup \{ p_\mu \}$.
    \item Aggregated reduced spaces: Construct the reduced space by adding both results to both spaces, i.e. $V_N^{pr, k + 1} = V_N^{du, k + 1} := V_N^{pr, k} \cup \{ u_\mu, p_\mu \}$.
\end{itemize}
The latter approach has the side effect that the NCD-corrected reduced and the standard reduced functionals coincide, hence giving up all advantages we might gain from pursuing an NCD-corrected approach.

In addition to the different ways one may construct the reduced spaces,~\cite[Subsection 4.4]{Keil2021} lists a short description of the three varying approaches including the standard approach in~\cite{Qian2017}, a semi NCD-corrected approach, and the ``fully'' NCD-corrected approach taken in~\cite{Keil2021}.
Distinctions between these methods mostly lie in the realm of order of estimation, being distinguished only by the increased computational complexities necessary for more accurate results.

To close, we shall make a few remarks on convergence of the adaptive trust-region reduced basis method.
In general, this is based on a few assumptions such as positivity of the functional $\mathcal{J}$, and the existence of trust-radii sufficiently large but in applications these are no hard constraints, and hence can be assumed satisfied.
It can then be shown that every accumulation point of the generated parameter sequence is an approximate first order critical point, cf.~\cite[Theorem 4.5]{Keil2021}.
A more robust proof has been given in~\cite{Banholzer2020}, though the method described there employs a projective Newton method instead of BFGS to attain a faster local rate of convergence.

\begin{algorithm}[H]\label{alg:AdaptTRRB}
    \caption{Adaptive TR-RB Algorithm, cf.~\cite[Algorithm 1]{Keil2021}}
    \KwData{Initial guess $\mu^{(0)}$, initial trust-radius $\delta^{(0)}$, rate of change and safeguard of the radius $\beta_1, \beta_2 \in \interval[open]{0}{1}$, tolerance for enlarging the radius $\eta$, tolerances for the BFGS method and the first order critical condition (FOC) $0 <\tau_{BFGS} \leq \tau_{FOC} \ll 1$}
    Set $k := 0, loop\_flag := true$\;
    \While{$loop\_flag$}{
        Compute $\mu^{(k + 1)}$ by the BFGS method\;
        \If{$\mathcal{J}_N^{(k)}(\mu^{(k + 1)}) + \Delta_{\mathcal{J}_N^{(k)}}(\mu^{(k + 1)}) < \mathcal{J}_N^{(k)}(\mu_{AGC}^{(k)})$}{
            Accept $\mu^{(k + 1)}$, update the reduced spaces at $\mu^{(k + 1)}$, and compute $\rho^{(k)}$\;\label{Update1}
            \If{$\rho^{(k)} \geq \eta$}{
                Enlarge the radius $\delta^{(k + 1)} := \beta_1^{-1} \delta^{(k)}$\;
            }
            \Else{
                Keep $\delta^{(k + 1)} := \delta^{(k)}$\;
            }
        }
        \ElseIf{$\mathcal{J}_N^{(k)}(\mu^{(k + 1)}) + \Delta_{\mathcal{J}_N^{(k)}}(\mu^{(k + 1)}) > \mathcal{J}_N^{(k)}(\mu_{AGC}^{(k)})$}{
            Reject $\mu^{(k + 1)}$, shrink the radius $\delta^{(k + 1)} := \beta_1 \delta^{(k)}$, and go to 3\;
        }
        \Else{
            Update the reduced spaces at $\mu^{(k)}$, and compute $\rho^{(k)}$\;\label{Update2}
            \If{$\mathcal{J}_N^{(k + 1)}(\mu^{(k + 1)}) \leq \mathcal{J}_N^{(k)}(\mu_{AGC}^{(k)})$}{
                Accept $\mu^{(k + 1)}$\;
                \If{$\rho^{(k)} \geq \eta$}{
                    Enlarge the radius\;
                }
                \Else{
                    Keep the radius\;
                }
            }
            \Else{
                Reject $\mu^{(k + 1)}$, shrink the radius, and go to 3\;
            }
        }
        \If{FOC is satifisfied for $\tau_{FOC}$}{
            $loop\_flag := false$\;
        }
        $k := k + 1$\;
    }
\end{algorithm}