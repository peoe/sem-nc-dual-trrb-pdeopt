\section{Theoretical Framework}

\subsection{Reduced Basis Method}

\begin{frame}{Full Order Model}
    \begin{block}{Standard Finite Element Formulation}
        For a finite element space find $u \in V$ such that
        \begin{equation*}
            a_\mu(u, v) = l_\mu(v) \quad \forall v \in V
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}{Reduced Basis Method, c.f.\@e.g.~\cite{Ohlberger2015}}
    \begin{block}{Idea}
        Construct an $N$-dimensional subspace $V_N \subseteq V$ of the full order model (FOM) space $V$.
    \end{block}

    \only<2>{
        Construction:
        \begin{itemize}
            \item Greedy Construction
            \item Discrete Empirical Interpolation
            \item Proper Orthogonal Decomposition
        \end{itemize}
    }
\end{frame}

\begin{frame}{Comparison FEM/RBM}
    \begin{block}{FE Formulation}
        For a finite element space find $u \in V$ such that
        \begin{equation*}
            a_\mu(u, v) = l_\mu(v) \quad \forall v \in V
        \end{equation*}
    \end{block}

    \begin{block}{RB Formulation}
        For a finite subspace $V_N \subseteq V$ find $u_N \in V_N$ such that
        \begin{equation*}
            a_\mu(u_N, v_N) = l_\mu(v_N) \quad \forall v_N \in V_N
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}{Comparison FEM/RBM}
    \begin{columns}
        \column{0.5\linewidth}
            \textbf{FEM} \\
            \begin{itemize}
                \item sparse system matrix
                \item large basis
                \only<2>{
                    \item no offline/online decomposition
                }
            \end{itemize}
        \column{0.5\linewidth}
            \textbf{RBM} \\
            \begin{itemize}
                \item dense system matrix
                \item small basis
                \only<2>{
                    \item offline/online decomposition
                }
            \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}{Offline/Online Decomposition}
    \begin{block}{Parameter Separability}
        For $u, v \in V_N, p_a, p_l \in \mathbb{N}$
        \begin{align*}
            a_\mu(u, v) &= \sum\limits_{p = 0}^{p_a} \theta_p^a(\mu) a_p(u, v), \\
            l_\mu(v) &= \sum\limits_{p = 0}^{p_l} \theta_p^l(\mu) l_p(v),
        \end{align*}
        where
        \begin{itemize}
            \item $\theta^a_i, \theta^l_i$ parameter functions, and
            \item $a_i, l_i$ \textbf{parameter independent}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Offline/Online Decomposition}
    \only<1>{
        \begin{block}{Offline Assembly}
            We get the \textbf{offline part}
            \begin{align*}
                \mathbb{A}_p^{i, j} &:= a_p(\varphi_i, \varphi_j), i = 1, \dots, N, p = 1, \dots, p_a, \\
                \mathbb{L}_p^i &:= l_p(\varphi_i), i = 1, \dots, N, p = 1, \dots, p_l,
            \end{align*}
            for the reduced basis $\{ \varphi_1, \dots, \varphi_N \}$ of $V_N$.
        \end{block}
    }
    \only<2>{
        \begin{block}{Offline Assembly}
            \vspace*{-13pt}
            \begin{align*}
                \mathbb{A}_p^{i, j} := a_p(\varphi_i, \varphi_j), \qquad \mathbb{L}_p^i := l_p(\varphi_i)
            \end{align*}
        \end{block}
        \begin{block}{Online Assembly}
            We get the \textbf{online part}
            \begin{align*}
                \mathbb{A} := \sum\limits_{p = 1}^{p_a} \theta_p^a(\mu) \mathbb{A}_p, \qquad \mathbb{L} := \sum\limits_{p = 1}^{p_l} \theta_p^l(\mu) \mathbb{L}_p,
            \end{align*}
            for the reduced basis $\{ \varphi_1, \dots, \varphi_N \}$ of $V_N$.
        \end{block}
    }
\end{frame}

\subsection{PDE Constrained Optimization}

\begin{frame}{Notes on Differentiability}
    \only<1, 2>{
        \begin{block}{Directional Derivative, c.f.~\cite{Hinze2009}}
            $F: X \rightarrow Y$ if
            \begin{equation*}
                dF(x)[h] := \lim\limits_{t \rightarrow 0} \frac{F(x + th) - F(x)}{t} \in Y
            \end{equation*}
            exists.
        \end{block}
    }
    \only<2, 3>{
        \begin{block}{G\^{a}teaux Derivative, c.f.~\cite{Hinze2009}}
            $F: X \rightarrow Y$ if $dF(x) \in \mathcal{L}(X, Y)$.
        \end{block}
    }
    \only<3>{
        \begin{block}{Fr\'{e}chet Derivative, c.f.~\cite{Hinze2009}}
            $F: X \rightarrow Y$ if for $\norm[x]{h} \rightarrow 0$ it holds that
            \begin{equation*}
                \norm[Y]{F(x + th) - F(x) - dF(x)[h]} = o(\norm[X]{h}).
            \end{equation*}
        \end{block}
    }
    \only<4>{
        \begin{block}{Bilinear Form}
            The chain rule also holds true:
            \begin{align*}
                d_\mu a_\mu(u_\mu, v_\mu) \cdot \nu &= \partial_\mu a_\mu(u_\mu, v_\mu) \cdot \nu + \partial_u a_\mu(u_\mu, v_\mu)[d_\nu u_\mu] + \partial_v a_\mu(u_\mu, v_\mu)[d_\nu v_\mu] \\
                &= \partial_\mu a_\mu(u_\mu, v_\mu) \cdot \nu + a_\mu(d_\nu u_\mu, v_\mu) + a_\mu(u_\mu, d_\nu v_\mu).
            \end{align*}
            $d_\nu u_\mu, d_\nu v_\mu$ are also called \textbf{sensitivities}.
        \end{block}
    }
\end{frame}

\begin{frame}{PDE Constrained Optimization}
    \begin{block}{Motivation}
        Parameter dependence in PDEs:
        \begin{itemize}
            \item material properties
            \item velocities
            \item heat sources/sinks
        \end{itemize}
    \end{block}
    \only<2>{
        \center{\textbf{How to determine optimal parameters?}}
    }
\end{frame}

\begin{frame}{PDE Constrained Optimization}
    \begin{block}{Problem Description, c.f.~\cite{Hinze2009}}
        We consider
        \begin{align*}
            &\min\limits_{\mu \in \mathcal{P}} \mathcal{J}(u, \mu) \qquad s.t.\\
            &e(u, \mu) = 0,
        \end{align*}
        where $e_\mu(u, v)$ encodes the PDE constraint
        \begin{equation*}
            e(u, \mu) := l_\mu(v) - a_\mu(u, v) \qquad \forall v \in V \text{ or } V_N
        \end{equation*}
    \end{block}
\end{frame}

\subsection{Trust-Region and BFGS Method}

\begin{frame}{Trust-Region Method}
    \begin{block}{Model function, c.f.~\cite{Kelley1999}}
        \only<1, 2>{Cost functional $\mathcal{J}$ too expensive to efficiently optimize.}
        \only<2>{
            \\~\\
            \textbf{Idea:} Replace with simpler function $m^{(k)}$ modelling $\mathcal{J}$ around some point $x^{(k)}$.
        }
        \only<3>{Replace $\mathcal{J}$ with simpler \textbf{model function} $m^{(k)}$ around some point $x^{(k)}$.}
    \end{block}
    \only<3>{
        \begin{block}{Trust-Region}
            Region where $m^{(k)}$ approximates $\mathcal{J}$ well:
            \begin{equation*}
                \Delta^{(k)} := \left\{ x \in X \; | \; \norm[X]{x - x^{(k)}} \leq \delta^{(k)} \right\}.
            \end{equation*}
            \begin{itemize}
                \item $\Delta^{(k)}$: \textbf{trust-region}
                \item $\delta^{(k)}$: \textbf{trust-radius}
            \end{itemize}
        \end{block}
    }
\end{frame}

\begin{frame}{Trust-Region Method}
    \begin{block}{Trust-Region Method}
        \textbf{Idea:} Optimize model function locally, i.e.\@
        \begin{align*}
            \argmin\limits_{\norm{s} \leq \delta^{(k)}} m^{(k)}(x^{(k)} + s) \\
            (s.t. \text{ constraints are satisfied}).
        \end{align*}
    \end{block}
    \only<2>{
        \textbf{Note:} This implies use of a local optimization algorithm!
    }
\end{frame}

\begin{frame}{Enrichment}
    \textbf{Problem:} Updating of the TR parameters
    \only<2, 3, 4, 5>{
        \begin{itemize}
            \item Enrich model function \only<2>{every iteration}\only<3, 4, 5>{if necessary}
            \only<4, 5>{
                \item Shrink trust-radius if necessary\only<5>{, enlarge trust-radius if possible}
            }
        \end{itemize}
    }
\end{frame}

\begin{frame}{BFGS Method}
    \begin{block}{Newton Method, c.f.~\cite{Nocedal2006}}
        \vspace*{-13pt}
        \begin{align*}
            x^{(k + 1)} := x^{(k)} + \kappa^{(k)} d^{(k)}, \\
            d^{(k)} := - {\mathcal{H}(x^{(k)})}^{-1} \nabla \mathcal{J}(x^{(k)})
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}{BFGS Method}
    \begin{block}{Quasi Newton Method, c.f.~\cite{Kelley1999, Nocedal2006}}
        \vspace*{-13pt}
        \begin{align*}
            x^{(k + 1)} := x^{(k)} + \kappa^{(k)} d^{(k)}, \\
            d^{(k)} := - {\mathcal{H}^{(k)}}^{-1} \nabla \mathcal{J}(x^{(k)}),
        \end{align*}
        $\mathcal{H}^{(k)}$ is symmetric, pos.\@ def.\@ matrix updated every iteration to approximate the true Hessian.
    \end{block}
\end{frame}

\begin{frame}{BFGS Method}
    \begin{block}{BFGS Update Formula}
        \vspace*{-13pt}
        \begin{align*}
            \mathcal{H}^{(k + 1)} &:= \mathcal{H}^{(k)} + \frac{(y^{(k)} - \mathcal{H}^{(k)} s^{(k)}){(y^{(k)} - \mathcal{H}^{(k)} s^{(k)})}^T}{{(y^{(k)} - \mathcal{H}^{(k)} s^{(k)})}^T s^{(k)}}, \\
            s^{(k)} &:= x^{(k + 1)} - x^{(k)}, \\
            y^{(k + 1)} &:= \nabla \mathcal{J}(x^{(k + 1)}) - \nabla \mathcal{J}(x^{(k)})
        \end{align*}
    \end{block}
\end{frame}