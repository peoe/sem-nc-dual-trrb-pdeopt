\section{Theoretical Framework}

\subsection{PDE Constrained Optimization}

\begin{frame}{PDE Constrained Optimization}
    \begin{block}{Problem Description, c.f.~\cite{Hinze2009}}
        We consider
        \begin{align*}
            &\min\limits_{\mu \in \mathcal{P}} \mathcal{J}(u, \mu) \qquad s.t.\\
            &e(u, \mu) = 0,
        \end{align*}
        where $e_\mu(u, v)$ encodes the PDE constraint
        \begin{equation*}
            e(u, \mu) := l_\mu(v) - a_\mu(u, v) \qquad \forall v \in V \text{ or } V_N
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}{Notes on Differentiability}
    \only<1>{
        \begin{block}{Directional Derivative, c.f.~\cite{Hinze2009}}
            \vspace*{-8pt}
            \[ dF(x)[h] := \lim\limits_{t \rightarrow 0} \frac{F(x + th) - F(x)}{t} \in Y \]
        \end{block}
        
        \begin{block}{G\^{a}teaux Derivative, c.f.~\cite{Hinze2009}}
            \vspace*{-8pt}
            \[ dF(x) \in \mathcal{L}(X, Y) \]
        \end{block}

        \begin{block}{Fr\'{e}chet Derivative, c.f.~\cite{Hinze2009}}
            \vspace*{-8pt}
            \[ \norm[Y]{F(x + th) - F(x) - dF(x)[h]} = o(\norm[X]{h}), \norm[X]{h} \rightarrow 0 \]
        \end{block}
    }
        
    \only<2>{
        \begin{block}{Bilinear Form}
            Chain rule:
            \begin{align*}
                d_\mu a_\mu(u_\mu, v_\mu) \cdot \nu &= \partial_\mu a_\mu(u_\mu, v_\mu) \cdot \nu + \partial_u a_\mu(u_\mu, v_\mu)[d_\nu u_\mu] + \partial_v a_\mu(u_\mu, v_\mu)[d_\nu v_\mu] \\
                &= \partial_\mu a_\mu(u_\mu, v_\mu) \cdot \nu + a_\mu(d_\nu u_\mu, v_\mu) + a_\mu(u_\mu, d_\nu v_\mu)
            \end{align*}
            $d_\nu u_\mu, d_\nu v_\mu$ are also called \textbf{sensitivities}.
        \end{block}
    }
\end{frame}

\subsection{Trust-Region and BFGS Method}

\begin{frame}{Trust-Region Method}
    \begin{block}{Model function, c.f.~\cite{Kelley1999}}
        \only<1, 2>{Cost functional $\mathcal{J}$ too expensive to efficiently optimize.}
        \only<2>{
            \\~\\
            \textbf{Idea:} Replace with simpler function $m^{(k)}$ modelling $\mathcal{J}$ around some point $x^{(k)}$.
        }
        \only<3>{Replace $\mathcal{J}$ with simpler \textbf{model function} $m^{(k)}$ around some point $x^{(k)}$.}
    \end{block}
    \only<3>{
        \begin{block}{Trust-Region}
            Region where $m^{(k)}$ approximates $\mathcal{J}$ well:
            \begin{equation*}
                \Delta^{(k)} := \left\{ x \in X \; | \; \norm[X]{x - x^{(k)}} \leq \delta^{(k)} \right\}.
            \end{equation*}
            \begin{itemize}
                \item $\Delta^{(k)}$: \textbf{trust-region}
                \item $\delta^{(k)}$: \textbf{trust-radius}
            \end{itemize}
        \end{block}
    }
\end{frame}

\begin{frame}{Trust-Region Method}
    \begin{block}{Trust-Region Method}
        \textbf{Idea:} Optimize model function locally, i.e.\@
        \begin{align*}
            \argmin\limits_{\norm{s} \leq \delta^{(k)}} m^{(k)}(x^{(k)} + s) \\
            (s.t. \text{ constraints are satisfied}).
        \end{align*}
    \end{block}
    \only<2>{
        \textbf{Note:} This implies use of a local optimization algorithm!
    }
\end{frame}

\begin{frame}{Enrichment}
    \textbf{Problem:} Updating of the TR parameters
    \only<2, 3, 4, 5>{
        \begin{itemize}
            \item Enrich model function \only<2>{every iteration}\only<3, 4, 5>{if necessary}
            \only<4, 5>{
                \item Shrink trust-radius if necessary\only<5>{, enlarge trust-radius if possible}
            }
        \end{itemize}
    }
\end{frame}

\begin{frame}{BFGS Method}
    \begin{block}{Newton Method, c.f.~\cite{Nocedal2006}}
        \vspace*{-13pt}
        \begin{align*}
            x^{(k + 1)} := x^{(k)} + \kappa^{(k)} d^{(k)}, \\
            d^{(k)} := - {\mathcal{H}(x^{(k)})}^{-1} \nabla \mathcal{J}(x^{(k)})
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}{BFGS Method}
    \begin{block}{Quasi Newton Method, c.f.~\cite{Kelley1999, Nocedal2006}}
        \vspace*{-13pt}
        \begin{align*}
            x^{(k + 1)} := x^{(k)} + \kappa^{(k)} d^{(k)}, \\
            d^{(k)} := - {\mathcal{H}^{(k)}}^{-1} \nabla \mathcal{J}(x^{(k)}),
        \end{align*}
        $\mathcal{H}^{(k)}$ is symmetric, pos.\@ def.\@ matrix updated every iteration to approximate the true Hessian.
    \end{block}
\end{frame}

\begin{frame}{BFGS Method}
    \begin{block}{BFGS Update Formula}
        \vspace*{-13pt}
        \begin{align*}
            \mathcal{H}^{(k + 1)} &:= \mathcal{H}^{(k)} + \frac{(y^{(k)} - \mathcal{H}^{(k)} s^{(k)}){(y^{(k)} - \mathcal{H}^{(k)} s^{(k)})}^T}{{(y^{(k)} - \mathcal{H}^{(k)} s^{(k)})}^T s^{(k)}}, \\
            s^{(k)} &:= x^{(k + 1)} - x^{(k)}, \\
            y^{(k + 1)} &:= \nabla \mathcal{J}(x^{(k + 1)}) - \nabla \mathcal{J}(x^{(k)})
        \end{align*}
    \end{block}
\end{frame}