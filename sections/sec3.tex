\section{ROM Optimality Systems}

In the following, we want to consider optimality conditions for the reduced problem and from there deduce error estimates for the reduced problem..
This will involve primal and dual spaces for the conditions, causing the easily computable gradients that one would usually use to derive error estimates to differ from the exact gradients.
Only a short overview of the ideas mentioned will be given here, but a longer description can be found in~\cite[Section 3]{Keil2021}.
We then move on to state two different approaches to estimating the true gradients in the reduced problem.
Combining all this, we finally establish error estimates on the inexact functionals and their gradients.

If we consider~\eqref{StateEq} for our FOM and ROM problems~\eqref{FOMEq} and~\eqref{ROMEq}, we can see that the state equation can be restated through
\begin{equation*}
    e(u_\mu, \mu) = l_\mu(\cdot) - a_\mu(u_\mu, \cdot) \in V' \text{ or } V_N'.
\end{equation*}
We now define the \textbf{primal residual} by $Res_\mu^{pr}(u)[v] := l_\mu(v) - a_\mu(u, v)$.
Following~\cite[Corollary 1.3]{Hinze2009}, we get the optimality conditions in~\cite[Proposition 2.9]{Keil2021}, then derive from these the following dual relation
\begin{equation*}
    a_\mu(v, p_\mu) = \partial_u \mathcal{J}(u_\mu, \mu)[v] = j_\mu(v) + 2 k_\mu(v, u_\mu) \quad \forall v \in V \text{ or } V_N,
\end{equation*}
and finally define the \textbf{dual residual} in terms of the difference in the dual optimality condition $Res_\mu^{du}(u_\mu, p_\mu)[v] := j_\mu(v) + 2 k_\mu(v, u_\mu) - a_\mu(v, p_\mu)$.
By the same optimality conditions, all locally optimal points of~\eqref{OptiProb} have to satisfy
\begin{equation}\label{PrimalEq}
    Res_\mu^{pr}(u_\mu)[v] = 0 \qquad \forall v \in V \text{ or } V_N^{pr},
\end{equation}
and
\begin{equation}\label{DualEq}
    Res_\mu^{du}(u_\mu, p_\mu)[v] = 0 \qquad \forall v \in V \text{ or } V_N^{du}.
\end{equation}

In this and the following sections, $u_\mu$ will always denote the primal solution $u$ for some parameter $\mu$, and similarly $p_\mu$ the dual solution.
If necessary, we will denote the ROM solutions by $u_{N, \mu}$ ($p_{N, \mu}$ respectively), while we will avoid additional indices for the FOM solutions.

\subsection{Standard Approach for a ROM Optimality System}

We now introduce a first method to obtain an inexact gradient as proposed in~\cite{Qian2017} and specifically the variant described in~\cite[Subsection 3.2]{Keil2021}.
This method is practical because it enables the approximation of the exact gradient for the reduced solutions.
One particuliarity of this idea as mentioned by~\cite{Qian2017} is that we can afterwards disregard the usual offline/online paradigm during the optimization and thus adapt the reduced approximation on the fly.

We note here that as mentioned in~\cite[Subsection 3.2]{Keil2021} the primal and the dual spaces are in general not the same, even the dimensions may be assumed to differ.
Hence, for the reduced functional $J_N(\mu) := \mathcal{J}(u_{N, \mu}, \mu)$, when considering the gradients (or rather their componentwise definitions) we get
\begin{align}\label{Grads}
    {\big( \nabla_\mu J_N(\mu) \big)}_i &:= \partial_u \mathcal{L}(u_\mu, \mu, p)[d_{\mu_i} u_\mu] + \partial_{\mu_i} \mathcal{L}(u_\mu, \mu, p) \qquad \forall p \in V_N^{du}, \tag*{(exact)} \\
    {\big( \tilde{\nabla}_\mu J_N(\mu) \big)}_i &:= \partial_{\mu_i} \mathcal{J}(u_\mu, \mu) + \partial_{\mu_i} Res_\mu^{pr}(u_\mu)[p_\mu], \tag*{(inexact)}
\end{align}
where in the first line of equations we considered the Lagrangian for the reduced version of~\eqref{SettingOpti} and apply $J_N(\mu) = \mathcal{L}(u_\mu, \mu, p) \; \forall p \in V_N^{du}$.
If we then consider $p = p_\mu$ in the first line and combine them with the second line we obtain
\begin{equation*}\label{RelationGrads}
    {\big( \nabla_\mu J_N(\mu) \big)}_i = \underbrace{\partial_u \mathcal{L}(u_\mu, \mu, p)[d_{\mu_i} u_\mu]}_{(\ast)} + {\big( \tilde{\nabla}_\mu J_N(\mu) \big)}_i.
\end{equation*}
The key insight here is that in general $(\ast) \neq 0$ because we are not actually considering all elements from $V$ for~\eqref{DualEq}, but just those from the reduced dual space.

\subsection{NCD-corrected Approach for a ROM Optimality System}

To extend on the result of the previous section, the NCD-corrected reduced functional in~\cite[Subsection 3.3]{Keil2021} contains one more step to attain improved error estimates.
Specifically, this involves the restriction of the Lagrangian in the dual variable.
We get
\begin{equation}\label{NCDFunctional}
    \mathcal{J}_N(\mu) := \mathcal{L}(u_\mu, \mu, p_\mu) = J_N(\mu) + Res_\mu^{pr}(u_\mu)[p_\mu].
\end{equation}
This is the functional we will be dealing with forthwith, and we define the reduced optimization problem
\begin{equation}\label{ReducProb}
    \min\limits_{\mu \in \mathcal{P}} \mathcal{J}_N(\mu).
\end{equation}

The key insight for the coming error estimates is that the following statement (cf.~\cite[Proposition 3.3]{Keil2021}) about the gradient of~\eqref{NCDFunctional} holds true:
\begin{proposition}\label{NCDGradientProp}
    We have
    \begin{equation*}\label{NCDGradient}
        {\big( \nabla_\mu \mathcal{J}_N(\mu) \big)}_i = \partial_{\mu_i} \mathcal{J}(u_\mu, \mu) + \partial_{\mu_i} Res_\mu^{pr}(u_\mu)[p_\mu + w_\mu] - Res_\mu^{du}(u_\mu, p_\mu)[z_\mu],
    \end{equation*}
    where $z_\mu \in V_N^{du}$ and $w_\mu \in V_N^{pr}$ respectively denote the solutions to the following problems:
    \begin{align*}
        a_\mu(z_\mu, q) &= -Res_\mu^{pr}(u_\mu)[q] &&\forall q \in V_N^{du}, \text{ and} \\
        a_\mu(v, w_\mu) &= Res_\mu^{du}(u_\mu, p_\mu)[v] - 2 k_\mu(z_\mu, v) &&\forall v \in V_N^{pr}.
    \end{align*}
\end{proposition}

Besides this classical approach we can also try and compute the gradient of the NCD-corrected functional~\eqref{NCDFunctional} by the sensitivities of the primal and dual reduced solutions (cf.~\cite[Proposition 3.9]{Keil2021}).
\begin{proposition}\label{NCDGradientSensProp}
    We have
    \begin{align*}\label{NCDGradientSens}
        {\big( \nabla_\mu \mathcal{J}_N(\mu) \big)}_i &= \partial_{\mu_i} \mathcal{J}(u_\mu, \mu) + \partial_{\mu_i} Res_\mu^{pr}(u_\mu)[p_\mu] \\
        &+ Res_\mu^{pr}(u_\mu)[d_{\mu_i} p_\mu] + Res_\mu^{du}(u_\mu, p_\mu)[d_{\mu_i} u_\mu].
    \end{align*}
\end{proposition}
Here, sensitivities mean the derivatives of these solutions w.r.t.\@ the parameters:
\begin{align*}
    a_\mu(d_\nu u_\mu, v) &= \partial_\mu Res_\mu^{pr}(u_\mu)[v] \cdot \nu &&\forall v \in V_N^{pr}, \text{ and} \\
    a_\mu(q, d_\nu p_\mu) &= \partial_\mu Res_\mu^{du}(u_\mu, p_\mu)[q] \cdot \nu + 2 k_\mu(q, d_\nu u_\mu) &&\forall q \in V_N^{du}.
\end{align*}

\subsection{\textit{A posteriori} error analysis}

The previous results in this section now enable us to take a look at \textit{a posteriori} error estimators.
Most of these involve comparisons of the ROM solution to the FOM solution, so we want to reiterate the notation in that $u_\mu$ will always denote the primal solution $u$ for some parameter $\mu$ of the FOM, and similarly $p_\mu$ the dual solution of the FOM, and $u_{N, \mu}$, $p_{N, \mu}$ the respective solutions of the ROM.\@
This part will then highlight two different approaches for obtaining the error estimates: the standard approach of RBM and the sensitivities approach.
As~\cite{Keil2021} does, we also want to emphasize that most of these results can only be computed efficiently by means of the assumption that the problems are offline/online decomposable.

To begin with we have the usual estimates we would expect from coercive problems.
Proofs may be found in~\cite{Rozza2008, Qian2017}.
\begin{align*}
    \norm{u_\mu - u_{N, \mu}} &\leq \Delta_{pr}(\mu) := {\alpha_\mu}^{-1} \norm{Res_\mu^{pr}(u_{N, \mu})}, \text{ and} \\
    \norm{p_\mu - p_{N, \mu}} &\leq \Delta_{du}(\mu) := {\alpha_\mu}^{-1} \big( 2 \gamma_{k_\mu} \Delta_{pr}(\mu) + \norm{Res_\mu^{du}(u_{N, \mu}, p_{N, \mu})} \big),
\end{align*}
where $\alpha_\mu$ denotes the coercivity constant of $a_\mu$, $\gamma_{k_\mu}$ stands for the modulus of continuity of $k_\mu$ and the norms of the residuals are to be understood as the norms of the respective linear functionals they represent.

In a similar fashion, we can obtain upper bounds on the errors between the exact and reduced functionals.
The first is a result for the simple inexact functional courtesy of~\cite[Theorem 4]{Qian2017}, while the second is the analogous statement for the NCD-corrected functional as seen in~\cite[Proposition 3.6]{Keil2021}.
We have
\begin{align*}
    \abs{\mathcal{J}(u_\mu, \mu) - J_N(\mu)} &\leq \Delta_{J_N}(\mu), \text{ and} \\
    \abs{\mathcal{J}(u_\mu, \mu) - \mathcal{J}_N(\mu)} &\leq \Delta_{\mathcal{J}_N}(\mu),
\end{align*}
where we define the respective $\Delta$s as follows
\begin{align*}
    \Delta_{J_N}(\mu) &:= \Delta_{pr}(\mu) \norm{Res_\mu^{du}(u_{N, \mu}, p_{N, \mu})} + {\Delta_{pr}(\mu)}^2 \gamma_{k_\mu} + \abs{Res_\mu^{pr}(u_{N, \mu})[p_{N, \mu}]}, \text{ and} \\
    \Delta_{\mathcal{J}_N}(\mu) &:= \Delta_{pr}(\mu) \norm{Res_\mu^{du}(u_{N, \mu}, p_{N, \mu})} + {\Delta_{pr}(\mu)}^2 \gamma_{k_\mu}.
\end{align*}
As mentioned by~\cite{Keil2021}, the second inequality is an improved version of the non-coorected definition avoiding lower order terms.

Continuing to gradients, we get a statement about the inexact and the NCD-corrected reduced gradients in accordance with~\cite[Proposition 3.8]{Keil2021}.
\begin{align*}
    \norm[2]{\nabla \mathcal{J}(u_\mu, \mu) - \tilde{\nabla} J_N(\mu)} &\leq \norm[2]{\Delta_{\tilde{\nabla} J_N}(\mu)}, \text{ and} \\
    \norm[2]{\nabla \mathcal{J}(u_\mu, \mu) - \tilde{\nabla} \mathcal{J}_N(\mu)} &\leq \norm[2]{\Delta_{\nabla \mathcal{J}_N (\mu)}^*},
\end{align*}
where the $\Delta$s are defined componentwise as follows
\begin{align*}
    {\left( \Delta_{\tilde{\nabla} J_N}(\mu) \right)}_i &:= 2 \Delta_{pr}(\mu) \gamma_{\partial_{\mu_i} k_\mu} \norm{u_{N, \mu}} + \Delta_{pr}(\mu) \left( \gamma_{\partial_{\mu_i} j_\mu} + \gamma_{\partial_{\mu_i} a_\mu} \norm{p_{N, \mu}} \right) \\
    &+ \Delta_{du}(\mu) \left( \gamma_{\partial_{\mu_i} l_\mu} + \gamma_{\partial_{\mu_i} a_\mu} \norm{p_{N, \mu}} \right) + \Delta_{pr}(\mu) \Delta_{du}(\mu) \gamma_{\partial_{\mu_i} a_\mu} \\
    &+ {\Delta_{pr}(\mu)}^2 \gamma_{\partial_{\mu_i} k_\mu}, \text{ and} \\
    {\left( \Delta_{\tilde{\nabla} \mathcal{J}_N}^*(\mu) \right)}_i &:= 2 \Delta_{pr}(\mu) \gamma_{\partial_{\mu_i} k_\mu} \norm{u_{N, \mu}} + \Delta_{pr}(\mu) \left( \gamma_{\partial_{\mu_i} l_\mu} + \gamma_{\partial_{\mu_i} a_\mu} \norm{p_{N, \mu}} \right) \\
    &+ \Delta_{du}(\mu) \left( \gamma_{\partial_{\mu_i} l_\mu} + \gamma_{\partial_{\mu_i} a_\mu} \norm{p_{N, \mu}} \right) + \Delta_{pr}(\mu) \Delta_{du}(\mu) \gamma_{\partial_{\mu_i} a_\mu} \\
    &+ {\alpha_\mu}^{-1} \left( \gamma_{\partial_{\mu_i} l_\mu} + \gamma_{\partial_{\mu_i} a_\mu} \norm{u_{N, \mu}} \right) \norm{Res_\mu^{du}(u_{N, \mu}, p_{N, \mu})} \\
    &+ {\alpha_\mu}^{-1} \norm{Res_\mu^{pr}(u_{N, \mu})} \left( \gamma_{\partial_{\mu_i} j_\mu} + 2 \gamma_{\partial_{\mu_i} k_\mu} \norm{u_{N, \mu}} + \gamma_{\partial_{\mu_i} a_\mu} \norm{p_{N, \mu}} \right) \\
    &+ 2 \gamma_{k_\mu} {\alpha_\mu}^{-2} \left( \gamma_{\partial_{\mu_i} l_\mu} + \gamma_{\partial_{\mu_i} a_\mu} \norm{u_{N, \mu}} \right) \norm{Res_\mu^{pr}(u_{N, \mu})} \\
    &+ {\Delta_{pr}(\mu)}^2 \gamma_{\partial_{\mu_i} k_\mu}. \\
\end{align*}
Reiterating the conclusion of~\cite{Keil2021} w.r.t.\@ this estimate the second inequality in general does not provide a better estimate than the simpler inexact reduced gradient.
Hence, we conclude the classical approach and look onward to the computation by means of sensitivities.

\todoinline{Write paragraphs on the estimation via sensitivities!}